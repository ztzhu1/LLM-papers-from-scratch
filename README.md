# LLM Papers from Scratch

## Introduction
This repository reimplements classic and cutting-edge (also interesting) Large Language Models (LLMs) and multimodal models from scratch, with hands-on tutorials and unit tests for each model. The goal is to help beginners understand key concepts of these models while learning by doing.

## Models
- [x] [ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929) - Applies the Transformer architecture to image patches for vision tasks.
- [x] [CLIP (Contrastive Language-Image Pre-training)](https://arxiv.org/abs/2103.00020) - Learns joint image-text representations using contrastive pretraining.
- [x] [Stable Diffusion](https://arxiv.org/abs/2112.10752) - Generates high-quality images from text prompts using Latent Diffusion Models (LDMs).
- [ ] [Qwen3MoE](https://arxiv.org/abs/2505.09388) - Selectively routes each input to a subset of expert networks for faster, more efficient inference.
- [x] [Z-Image](https://arxiv.org/abs/2511.22699) -Generates images based on the Scalable Single-Stream Diffusion Transformer (S3-DiT), which processes text and image tokens together in one transformer, avoiding cross-attention.
- [ ] [Wan](https://arxiv.org/abs/2503.20314) - An open video generative model built on diffusion transformers with spatio-temporal VAE.
- [ ] [DeepSeek-OCR](https://arxiv.org/abs/2510.18234) - A vision‑based OCR model that compresses high‑resolution pages into compact vision tokens and decodes them to recover text with high precision, enabling efficient long‑context document understanding.
- [ ] ...

## Tutorials and tests
Step-by-step tutorials and unit tests for each model are coming soon.
